{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gogog01-29-2021/241102-FBAQuant-1st-Capstone/blob/main/20241220.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install QuantLib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpHzXRLfxjYV",
        "outputId": "2dbfbbed-df13-4949-e68f-9cc23931dfc3"
      },
      "id": "XpHzXRLfxjYV",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: QuantLib in /usr/local/lib/python3.10/dist-packages (1.36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "006e21bc-4090-47f4-a18f-3f9c798e2375",
      "metadata": {
        "id": "006e21bc-4090-47f4-a18f-3f9c798e2375"
      },
      "outputs": [],
      "source": [
        "import QuantLib as ql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b8cea3e4-95f1-4564-babf-e581d3d14ee4",
      "metadata": {
        "id": "b8cea3e4-95f1-4564-babf-e581d3d14ee4"
      },
      "outputs": [],
      "source": [
        "def option_pricing(today, expiry):\n",
        "    date = today\n",
        "    ql.Settings.instance().evaluationDate = date\n",
        "    calendar = ql.SouthKorea()\n",
        "    dayCount = ql.ActualActual(ql.ActualActual.ISDA)\n",
        "\n",
        "    # SimpleQuote Objects\n",
        "    underlying_qt = ql.SimpleQuote(350) #기초자산 초기 가격\n",
        "    dividend_qt = ql.SimpleQuote(0.0) # 배당\n",
        "    riskfreerate_qt = ql.SimpleQuote(0.01)  #무위험 이자율\n",
        "    volatility_qt = ql.SimpleQuote(0.2)\n",
        "\n",
        "    # Quote Handle Objects\n",
        "    u_qhd = ql.QuoteHandle(underlying_qt)\n",
        "    d_qhd = ql.QuoteHandle(dividend_qt)\n",
        "    r_qhd = ql.QuoteHandle(riskfreerate_qt)\n",
        "    v_qhd = ql.QuoteHandle(volatility_qt)\n",
        "\n",
        "    # Term Structure Objects\n",
        "    r_ts = ql.FlatForward(date, r_qhd, dayCount) # 무위험 이자율 flatforward\n",
        "    d_ts = ql.FlatForward(date, d_qhd, dayCount) # 배당률 flatforward\n",
        "    v_ts = ql.BlackConstantVol(date, calendar, v_qhd, dayCount) # 변동성 ??\n",
        "\n",
        "    # Term structure Handle Objects\n",
        "    r_thd = ql.YieldTermStructureHandle(r_ts)\n",
        "    d_thd = ql.YieldTermStructureHandle(d_ts)\n",
        "    v_thd = ql.BlackVolTermStructureHandle(v_ts)\n",
        "\n",
        "    # Process & Engine\n",
        "    process = ql.BlackScholesMertonProcess(u_qhd, d_thd, r_thd, v_thd)\n",
        "    engine = ql.AnalyticEuropeanEngine(process)\n",
        "\n",
        "    # Option Objects\n",
        "    option_type = ql.Option.Put\n",
        "    K = 360\n",
        "    expiry_date = expiry\n",
        "    exercise = ql.EuropeanExercise(expiry_date)\n",
        "    payoff = ql.PlainVanillaPayoff(option_type, K)\n",
        "    option = ql.VanillaOption(payoff, exercise)\n",
        "\n",
        "    option.setPricingEngine(engine)\n",
        "\n",
        "    return option.NPV()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "bc722b08-f277-4434-a44e-5239a73c6423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc722b08-f277-4434-a44e-5239a73c6423",
        "outputId": "4be05bf7-b8cd-45e8-e26e-4ac83fb3e3cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.402475043536924"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "option_pricing(ql.Date(22,5,2022), ql.Date(22,11,2022))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e148c17b-c0d4-4b8d-958f-937d6618602d",
      "metadata": {
        "id": "e148c17b-c0d4-4b8d-958f-937d6618602d"
      },
      "outputs": [],
      "source": [
        "#Underlying Path\n",
        "import QuantLib as ql\n",
        "import numpy as np\n",
        "\n",
        "# 설정할 날짜\n",
        "date = ql.Date(22, 10, 2024)\n",
        "ql.Settings.instance().evaluationDate = date\n",
        "calendar = ql.SouthKorea()\n",
        "dayCount = ql.ActualActual(ql.ActualActual.ISDA)\n",
        "\n",
        "# 초기 파라미터 설정\n",
        "S0 = 350  # 초기 주가\n",
        "mu = 0.05  # 기대 수익률\n",
        "sigma = 0.2  # 변동성\n",
        "r = 0.01  # 무위험 이자율\n",
        "dividend = 0.0  # 배당률\n",
        "T = 1.0  # 만기 시간 (1년)\n",
        "dt = 1 / 252  # 일일 간격 (252 거래일 기준)\n",
        "N = int(T / dt)  # 총 시뮬레이션 스텝 수\n",
        "M = 100  # 시뮬레이션 횟수\n",
        "\n",
        "# 초기 주가와 무위험 이자율을 직접 사용하여 GeometricBrownianMotionProcess 설정\n",
        "process = ql.GeometricBrownianMotionProcess(S0, r, sigma)\n",
        "\n",
        "# 난수 생성기\n",
        "rng = ql.UniformRandomGenerator(seed=0)\n",
        "uniform_rng = ql.UniformRandomSequenceGenerator(N, rng)\n",
        "gaussian_rng = ql.GaussianRandomSequenceGenerator(uniform_rng)\n",
        "\n",
        "# 경로 생성기\n",
        "path_generator = ql.GaussianPathGenerator(process, T, N, gaussian_rng, False)\n",
        "\n",
        "# 시뮬레이션 저장\n",
        "simulations = []\n",
        "\n",
        "for _ in range(M):\n",
        "    sample_path = path_generator.next()\n",
        "    path = sample_path.value()\n",
        "    # path는 float 값의 리스트이므로, 직접 추가\n",
        "    simulations.append([path[i] for i in range(len(path))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a1af6cf0-a490-46b6-9a2e-93306524c30c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1af6cf0-a490-46b6-9a2e-93306524c30c",
        "outputId": "85943493-423b-480a-b4b3-57fa745823b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulations data shape: (100, 253)\n",
            "Sample path (first simulation): [350.         347.61805758 350.44179516 349.54165088 349.82416241\n",
            " 343.19535271 349.99342123 349.01581316 344.76272204 345.16878391]\n"
          ]
        }
      ],
      "source": [
        "simulations = np.array(simulations)\n",
        "# Check the dimensions and a sample of the generated data\n",
        "print(\"Simulations data shape:\", simulations.shape)\n",
        "print(\"Sample path (first simulation):\", simulations[0][:10])  # Display first 10 values of the first simulation path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "import QuantLib as ql\n",
        "import numpy as np\n",
        "\n",
        "# Simulation parameters for QuantLib\n",
        "date = ql.Date(22, 10, 2024)\n",
        "ql.Settings.instance().evaluationDate = date\n",
        "calendar = ql.SouthKorea()\n",
        "dayCount = ql.ActualActual(ql.ActualActual.ISDA)\n",
        "\n",
        "S0 = 350  # Initial stock price\n",
        "mu = 0.05  # Expected return\n",
        "sigma = 0.2  # Volatility\n",
        "r = 0.01  # Risk-free rate\n",
        "T = 1.0  # Maturity in years\n",
        "dt = 1 / 252  # Daily intervals (252 trading days in a year)\n",
        "N = int(T / dt)  # Total steps\n",
        "M = 100  # Number of simulation paths\n",
        "\n",
        "# Initialize the GBM process for the underlying price\n",
        "process = ql.GeometricBrownianMotionProcess(S0, r, sigma)\n",
        "rng = ql.UniformRandomGenerator(seed=0)\n",
        "uniform_rng = ql.UniformRandomSequenceGenerator(N, rng)\n",
        "gaussian_rng = ql.GaussianRandomSequenceGenerator(uniform_rng)\n",
        "path_generator = ql.GaussianPathGenerator(process, T, N, gaussian_rng, False)\n",
        "\n",
        "# Run simulations for underlying price paths\n",
        "simulations = []\n",
        "for _ in range(M):\n",
        "    sample_path = path_generator.next()\n",
        "    path = sample_path.value()\n",
        "    simulations.append([path[i] for i in range(len(path))])\n",
        "simulations = np.array(simulations)\n",
        "\n",
        "# Function to calculate option price using Black-Scholes model\n",
        "def calculate_option_price(S, K=350, T=1.0, r=0.01, sigma=0.2):\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    option_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "    return option_price\n",
        "\n",
        "# Initialize state for each path in the simulations\n",
        "def initialize_state_from_simulation(simulations, option_weight=0.5):\n",
        "    initial_states = []\n",
        "    for path in simulations:\n",
        "        underlying_price = path[0]  # Use first price in path as initial Underlying Price\n",
        "        option_price = calculate_option_price(underlying_price)  # Compute initial Option Price\n",
        "        underlying_weight = 1 - option_weight  # Set Underlying Weight as complement to Option Weight\n",
        "        state = [option_weight, underlying_weight, option_price, underlying_price]\n",
        "        initial_states.append(state)\n",
        "    return initial_states\n",
        "\n",
        "# Generate initial states\n",
        "initial_states = initialize_state_from_simulation(simulations)\n",
        "\n",
        "# Check the structure of initial states\n",
        "print(\"Initial States Sample:\", initial_states[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmK9uM0ly67b",
        "outputId": "8d95eb52-5915-4840-ce11-25be9ee5f968"
      },
      "id": "hmK9uM0ly67b",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial States Sample: [[0.5, 0.5, 29.516615415383598, 350.0], [0.5, 0.5, 29.516615415383598, 350.0], [0.5, 0.5, 29.516615415383598, 350.0], [0.5, 0.5, 29.516615415383598, 350.0], [0.5, 0.5, 29.516615415383598, 350.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class DeltaHedgeEnv(gym.Env):\n",
        "    def __init__(self, simulations, option_weight=0.5):\n",
        "        super(DeltaHedgeEnv, self).__init__()\n",
        "\n",
        "        # Store simulations and initial weights\n",
        "        self.simulations = simulations\n",
        "        self.option_weight = option_weight  # Initial option weight\n",
        "        self.underlying_weight = 1 - option_weight  # Complementary underlying weight\n",
        "\n",
        "        # Action space for DDPG: continuous range [-0.1, 0.1]\n",
        "        self.action_space = gym.spaces.Box(low=-0.1, high=0.1, shape=(1,), dtype=np.float32)\n",
        "\n",
        "        # Observation space for state: [Option Weight, Underlying Weight, Option Price, Underlying Price]\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n",
        "\n",
        "        # Initialize state variables\n",
        "        self.day_index = 0\n",
        "        self.sim_index = 0\n",
        "        self.P0 = None  # Initial portfolio value for the episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset at start of episode: Randomly choose a simulation path and reset day index\n",
        "        self.sim_index = np.random.randint(len(self.simulations))\n",
        "        self.day_index = 0\n",
        "        underlying_price = self.simulations[self.sim_index][self.day_index]\n",
        "        option_price = calculate_option_price(underlying_price)\n",
        "\n",
        "        # Initialize ef: [Option Weight, Underlying Weight, Option Price, Underlying Price]\n",
        "        self.num_options=100\n",
        "        self.portfolio_value=self.num_options*option_price\n",
        "        underlying_position=0 #Assume Starting with no underyling asset?\n",
        "        self.state = [underlying_position,self.num_options,self.portfolio_value,option_price,underlying_price]\n",
        "\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply the action to adjust the option weight\n",
        "        hedge_adjustment = action[0]\n",
        "        underlying_position=self.state[0]+hedge_adjustment\n",
        "        self.state[0]=underlying_position\n",
        "        # Advance to the next time step in the selected simulation path\n",
        "        self.day_index += 1\n",
        "        done = self.day_index >= len(self.simulations[self.sim_index])  # Check if episode is done\n",
        "\n",
        "        # Get next day's underlying price and calculate option price\n",
        "        if not done:\n",
        "            underlying_price_next = self.simulations[self.sim_index][self.day_index]\n",
        "            option_price_next = calculate_option_price(underlying_price_next)\n",
        "\n",
        "            portfolio_value_next=(underlying_position*underlying_price_next+self.num_options*option_price_next)\n",
        "\n",
        "            reward = portfolio_value_next-self.state[2] #self.calculate_reward(self.state, option_price_next, underlying_price_next)\n",
        "            self.state = [underlying_position,self.num_options,portfolio_value_next,option_price_next,underlying_price_next]\n",
        "        else:\n",
        "            reward = 0  # No reward if episode is done\n",
        "\n",
        "        return np.array(self.state), reward, done, {}\n",
        "\n",
        "    def calculate_reward(self, portfolio_value_next, portfolio_value_t):#self, state, option_price_next, underlying_price_next):\n",
        "        stability_reward=-abs(portfolio_value_next-portfolio_value_t)\n",
        "        if portfolio_value_next>0:\n",
        "            growth_reward=np.log(portfolio_value_next/portfolio_value_t)\n",
        "        else:\n",
        "            growth_reward=-np.inf\n",
        "        return 0.5*stability_reward+0.5*growth_reward\n",
        "        # Calculate stability reward\n",
        "        #balance_reward=-abs(self.option_weight-0.5)\n",
        "    # Calculate portfolio value\n",
        "    #     portfolio_value_t = self.state[2] * self.option_weight + self.state[3] * self.underlying_weight\n",
        "    #     portfolio_value_next = option_price_next * self.option_weight + underlying_price_next * self.underlying_weight\n",
        "\n",
        "    # # Stability term (as before)\n",
        "    #     delta_portfolio = portfolio_value_next - portfolio_value_t\n",
        "    #     delta_underlying = underlying_price_next - self.state[3]\n",
        "    #     if abs(delta_underlying) > 1e-6:\n",
        "    #         stability_reward = -abs(delta_portfolio / delta_underlying)\n",
        "    #     else:\n",
        "    #         stability_reward = 0\n",
        "\n",
        "    # # Return term based on log portfolio growth (Kelly criterion approximation)\n",
        "    #     if portfolio_value_next > 0 and portfolio_value_t > 0:\n",
        "    #         return_reward = np.log(portfolio_value_next / portfolio_value_t)\n",
        "    #     else:\n",
        "    #         return_reward = -np.inf  # Penalize if portfolio value is non-positive\n",
        "\n",
        "    # # Balance reward as in the previous section\n",
        "    #     balance_reward = -abs(self.option_weight - 0.5)\n",
        "\n",
        "    # # Combine all terms with respective weights\n",
        "    #     reward = stability_reward + 0.1 * balance_reward + 0.2 * return_reward\n",
        "    #     return reward\n",
        "\n"
      ],
      "metadata": {
        "id": "FLvDBvnV200t"
      },
      "id": "FLvDBvnV200t",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.l1(state))\n",
        "        x = torch.relu(self.l2(x))\n",
        "        action = self.max_action * torch.tanh(self.l3(x))\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.l1(torch.cat([state, action], 1)))\n",
        "        x = torch.relu(self.l2(x))\n",
        "        return self.l3(x)\n"
      ],
      "metadata": {
        "id": "V9-nWmutCYnY"
      },
      "id": "V9-nWmutCYnY",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=100000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.max_action = max_action\n",
        "        self.discount = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(self, batch_size=64):\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).reshape(-1, 1).to(device)\n",
        "\n",
        "        # Critic training\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.actor_target(next_states)\n",
        "            target_q = self.critic_target(next_states, next_actions)\n",
        "            target_q = rewards + (1 - dones) * self.discount * target_q\n",
        "\n",
        "        current_q = self.critic(states, actions)\n",
        "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor training\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Update target networks\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
      ],
      "metadata": {
        "id": "cSIcIHSMCbLD"
      },
      "id": "cSIcIHSMCbLD",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_ddpg(env, agent, episodes=1000, batch_size=64, save_interval=100, save_path='./ddpg_model'):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(agent.replay_buffer.buffer) > batch_size:\n",
        "                agent.train(batch_size)\n",
        "\n",
        "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "        # Save model at intervals\n",
        "        if (episode + 1) % save_interval == 0:\n",
        "            save_model(agent, save_path, episode + 1)\n",
        "\n",
        "def save_model(agent, path, episode):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    torch.save(agent.actor.state_dict(), os.path.join(path, f'actor_{episode}.pth'))\n",
        "    torch.save(agent.critic.state_dict(), os.path.join(path, f'critic_{episode}.pth'))\n"
      ],
      "metadata": {
        "id": "S1QYBD0OCnb9"
      },
      "id": "S1QYBD0OCnb9",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from google.colab import files\n",
        "# Set device for computation (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = DeltaHedgeEnv(simulations)\n",
        "\n",
        "# Define state and action dimensions based on the environment's spaces\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "# Instantiate the DDPG agent\n",
        "agent = DDPGAgent(state_dim, action_dim, max_action)\n",
        "\n",
        "# Filepath for saving and downloading checkpoints\n",
        "checkpoint_path = '/content/ddpg_checkpoint.pth'\n",
        "final_model_path = '/content/ddpg_final_model.pth'\n",
        "\n",
        "# Function to save and download the model checkpoint\n",
        "def save_checkpoint(agent, episode, filepath=checkpoint_path):\n",
        "    torch.save({\n",
        "        'actor_state_dict': agent.actor.state_dict(),\n",
        "        'critic_state_dict': agent.critic.state_dict(),\n",
        "        'target_actor_state_dict': agent.actor_target.state_dict(),\n",
        "        'target_critic_state_dict': agent.critic_target.state_dict(),\n",
        "        'optimizer_actor_state_dict': agent.actor_optimizer.state_dict(),\n",
        "        'optimizer_critic_state_dict': agent.critic_optimizer.state_dict(),\n",
        "        'episode': episode,\n",
        "    }, filepath)\n",
        "    print(f\"Checkpoint saved at episode {episode}.\")\n",
        "    # Download checkpoint file\n",
        "    files.download(filepath)\n",
        "    print(\"Checkpoint downloaded.\")\n",
        "\n",
        "# Function to load the model checkpoint if it exists\n",
        "def load_checkpoint(agent, filepath=checkpoint_path):\n",
        "    if os.path.exists(filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
        "        agent.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "        agent.target_actor.load_state_dict(checkpoint['actor_target_state_dict'])\n",
        "        agent.target_critic.load_state_dict(checkpoint['critic_target_state_dict'])\n",
        "        agent.actor_optimizer.load_state_dict(checkpoint['optimizer_actor_state_dict'])\n",
        "        agent.critic_optimizer.load_state_dict(checkpoint['optimizer_critic_state_dict'])\n",
        "        episode = checkpoint['episode']\n",
        "        print(f\"Checkpoint loaded, resuming from episode {episode}\")\n",
        "        return episode\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from episode 0\")\n",
        "        return 0\n",
        "\n",
        "# Load from checkpoint if available\n",
        "start_episode = load_checkpoint(agent)\n",
        "\n",
        "# Training loop with periodic checkpoint saving and interruption handling\n",
        "try:\n",
        "    for episode in range(start_episode, 1000):  # Run for the specified number of episodes\n",
        "        train_ddpg(env, agent, episodes=1)  # Train for one episode\n",
        "\n",
        "        # Save checkpoint every 100 episodes\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            save_checkpoint(agent, episode + 1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Save checkpoint if training is interrupted\n",
        "    print(\"Training interrupted. Saving current progress...\")\n",
        "    save_checkpoint(agent, episode)\n",
        "\n",
        "finally:\n",
        "    # Save the final model at the end of training and download it\n",
        "    torch.save(agent.actor.state_dict(), final_model_path)\n",
        "    print(f\"Final model saved at {final_model_path}\")\n",
        "    files.download(final_model_path)\n",
        "    print(\"Final model downloaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8qmMHy6iI5Ot",
        "outputId": "04b38f57-db86-48d7-a813-e86e575302aa"
      },
      "id": "8qmMHy6iI5Ot",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting from episode 0\n",
            "Episode 1, Reward: -7504.677837247262\n",
            "Episode 1, Reward: -8419.243935016302\n",
            "Episode 1, Reward: -3987.7548608606808\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -8656.202772107066\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -5282.507448661553\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -6815.183636082178\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -5452.451117881712\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -9607.632337850082\n",
            "Episode 1, Reward: -9573.082571576891\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -5282.507448661553\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -9523.362735591138\n",
            "Episode 1, Reward: -7900.337909496468\n",
            "Episode 1, Reward: -8806.69759563691\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -5452.451117881712\n",
            "Episode 1, Reward: -7504.677837247262\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -9624.133636724428\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -9602.633939294712\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -8022.824932526093\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -5452.451117881712\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -8766.909289728528\n",
            "Episode 1, Reward: -7172.655785126776\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Episode 1, Reward: -9212.215779347436\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -7172.655785126776\n",
            "Episode 1, Reward: -5321.18398143517\n",
            "Episode 1, Reward: -9625.991083594125\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -9568.339676272963\n",
            "Episode 1, Reward: -9539.956439176085\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -3987.7548608606808\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -9623.543411047109\n",
            "Episode 1, Reward: -8504.188154609514\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -9212.215779347436\n",
            "Episode 1, Reward: -9605.654061218764\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -8504.188154609514\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -5452.451117881712\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -9551.360558140066\n",
            "Episode 1, Reward: -5321.18398143517\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -8913.28672568171\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -9623.543411047109\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -9505.340149381023\n",
            "Episode 1, Reward: -9605.654061218764\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -5321.18398143517\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -8880.237066764861\n",
            "Episode 1, Reward: -5701.129927771363\n",
            "Episode 1, Reward: -9391.361205013043\n",
            "Checkpoint saved at episode 100.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c4877f05-72f2-4a50-9704-3b1ee1e97a83\", \"ddpg_checkpoint.pth\", 2185058)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint downloaded.\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -9568.339676272963\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -8753.739274280979\n",
            "Episode 1, Reward: -9561.059338297171\n",
            "Episode 1, Reward: -7172.06310620106\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -9551.360558140066\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -7504.677837247262\n",
            "Episode 1, Reward: -9607.632337850082\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -9473.400897957024\n",
            "Episode 1, Reward: -5701.129927771363\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -8806.69759563691\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Episode 1, Reward: -9338.542391332276\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -9605.654061218764\n",
            "Episode 1, Reward: -8419.243935016302\n",
            "Episode 1, Reward: -8766.909289728528\n",
            "Episode 1, Reward: -8231.446891001942\n",
            "Episode 1, Reward: -9384.84882013982\n",
            "Episode 1, Reward: -8947.063839563529\n",
            "Episode 1, Reward: -9625.613829679169\n",
            "Episode 1, Reward: -9555.137242769535\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -9551.360558140066\n",
            "Episode 1, Reward: -7552.90147469382\n",
            "Episode 1, Reward: -7172.06310620106\n",
            "Episode 1, Reward: 2935.088773348659\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -9338.542391332276\n",
            "Episode 1, Reward: -8947.063839563529\n",
            "Episode 1, Reward: -9597.597171789705\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -8129.340138709713\n",
            "Episode 1, Reward: -9601.171886706361\n",
            "Episode 1, Reward: -8536.731747156358\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: 2922.130714485931\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -8753.739274280979\n",
            "Episode 1, Reward: -8231.446891001942\n",
            "Episode 1, Reward: -8022.824932526093\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -9607.632337850082\n",
            "Episode 1, Reward: -9159.327857603592\n",
            "Episode 1, Reward: -7719.934177210235\n",
            "Episode 1, Reward: -8022.824932526093\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -8419.243935016302\n",
            "Episode 1, Reward: -8880.237066764861\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -9602.633939294712\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -9391.361205013043\n",
            "Episode 1, Reward: -8947.063839563529\n",
            "Episode 1, Reward: 2935.088773348659\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -298.4103027114379\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -7172.06310620106\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -9566.451891670606\n",
            "Episode 1, Reward: 812.6623583184264\n",
            "Episode 1, Reward: 2922.130714485931\n",
            "Episode 1, Reward: -9625.613829679169\n",
            "Episode 1, Reward: -9601.171886706361\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -9052.888382956215\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -7504.677837247262\n",
            "Episode 1, Reward: -9573.082571576891\n",
            "Episode 1, Reward: -9605.654061218764\n",
            "Episode 1, Reward: -6815.183636082178\n",
            "Episode 1, Reward: 2935.088773348659\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -9568.339676272963\n",
            "Episode 1, Reward: -9566.451891670606\n",
            "Episode 1, Reward: -8129.340138709713\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -9623.543411047109\n",
            "Episode 1, Reward: -9127.565227367097\n",
            "Episode 1, Reward: -9623.543411047109\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -8913.28672568171\n",
            "Checkpoint saved at episode 200.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_820a748f-a278-4917-808f-8986c478b2ba\", \"ddpg_checkpoint.pth\", 2185058)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint downloaded.\n",
            "Episode 1, Reward: -8504.188154609514\n",
            "Episode 1, Reward: -9596.763848005843\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -6665.575437670759\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -9511.768415837909\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -9473.400897957024\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -9568.339676272963\n",
            "Episode 1, Reward: -9561.059338297171\n",
            "Episode 1, Reward: -9391.361205013043\n",
            "Episode 1, Reward: -9611.62919145043\n",
            "Episode 1, Reward: -9539.956439176085\n",
            "Episode 1, Reward: -9611.62919145043\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -8231.446891001942\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: 2935.088773348659\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -8947.063839563529\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -5452.451117881712\n",
            "Episode 1, Reward: -9551.360558140066\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -9159.327857603592\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -8419.243935016302\n",
            "Episode 1, Reward: -1973.0839606149275\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -9338.542391332276\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -6665.575437670759\n",
            "Episode 1, Reward: -9601.171886706361\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -9384.84882013982\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -5282.507448661553\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -9052.888382956215\n",
            "Episode 1, Reward: 2935.088773348659\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -9391.361205013043\n",
            "Episode 1, Reward: -9473.400897957024\n",
            "Episode 1, Reward: -8231.446891001942\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -8129.340138709713\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -5225.0844143087\n",
            "Episode 1, Reward: -9212.215779347436\n",
            "Episode 1, Reward: -9625.991083594125\n",
            "Episode 1, Reward: 812.6623583184264\n",
            "Episode 1, Reward: -5321.18398143517\n",
            "Episode 1, Reward: -9625.613829679169\n",
            "Episode 1, Reward: -9511.768415837909\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -8766.909289728528\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -298.4103027114379\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -9607.632337850082\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Episode 1, Reward: -8753.739274280979\n",
            "Episode 1, Reward: -9539.956439176085\n",
            "Episode 1, Reward: -9623.543411047109\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -8323.255305307826\n",
            "Episode 1, Reward: -8806.69759563691\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -8753.739274280979\n",
            "Episode 1, Reward: -8947.063839563529\n",
            "Episode 1, Reward: -7172.655785126776\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -9611.62919145043\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Episode 1, Reward: -9551.360558140066\n",
            "Episode 1, Reward: -9212.215779347436\n",
            "Episode 1, Reward: -9566.451891670606\n",
            "Episode 1, Reward: -9473.400897957024\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -5701.129927771363\n",
            "Episode 1, Reward: -9553.265194014584\n",
            "Checkpoint saved at episode 300.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7c5651f-b576-4282-85ee-46c678775043\", \"ddpg_checkpoint.pth\", 2185058)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint downloaded.\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -8536.731747156358\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -9597.597171789705\n",
            "Episode 1, Reward: -7504.677837247262\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -9625.613829679169\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -9625.991083594125\n",
            "Episode 1, Reward: -9573.082571576891\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -5701.129927771363\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -8766.909289728528\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -9185.319257319006\n",
            "Episode 1, Reward: -8022.824932526093\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -8349.985985176643\n",
            "Episode 1, Reward: -8323.255305307826\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -9566.451891670606\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -8740.508576579876\n",
            "Episode 1, Reward: -8323.255305307826\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -8022.824932526093\n",
            "Episode 1, Reward: 812.6623583184264\n",
            "Episode 1, Reward: -7552.90147469382\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -9621.949982704582\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -9384.84882013982\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -8913.28672568171\n",
            "Episode 1, Reward: -9573.082571576891\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -8753.739274280979\n",
            "Episode 1, Reward: -9546.25015677069\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -5225.0844143087\n",
            "Episode 1, Reward: 2922.130714485931\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -5225.0844143087\n",
            "Episode 1, Reward: -7719.934177210235\n",
            "Episode 1, Reward: -9052.888382956215\n",
            "Episode 1, Reward: -3987.7548608606808\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -8536.731747156358\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -8129.340138709713\n",
            "Episode 1, Reward: -5321.18398143517\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -9555.137242769535\n",
            "Episode 1, Reward: -8799.996746136938\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -8388.413405043935\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -9601.171886706361\n",
            "Episode 1, Reward: -9555.137242769535\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -9596.763848005843\n",
            "Episode 1, Reward: -1924.3263063489858\n",
            "Episode 1, Reward: -8041.462699458167\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -6815.183636082178\n",
            "Episode 1, Reward: -9625.991083594125\n",
            "Episode 1, Reward: -2179.731923034248\n",
            "Episode 1, Reward: -9597.597171789705\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -9473.400897957024\n",
            "Episode 1, Reward: -5627.094966029047\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -9242.100411474486\n",
            "Episode 1, Reward: -9605.654061218764\n",
            "Episode 1, Reward: -9596.42455129257\n",
            "Episode 1, Reward: -8880.237066764861\n",
            "Episode 1, Reward: -8096.421567297562\n",
            "Episode 1, Reward: -8958.881654497425\n",
            "Episode 1, Reward: -537.423802847064\n",
            "Episode 1, Reward: -7552.90147469382\n",
            "Episode 1, Reward: -7715.090398676058\n",
            "Episode 1, Reward: -3987.7548608606808\n",
            "Checkpoint saved at episode 400.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7fd38637-1e93-4943-b57b-78170e6d84ae\", \"ddpg_checkpoint.pth\", 2185058)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint downloaded.\n",
            "Episode 1, Reward: -7719.934177210235\n",
            "Episode 1, Reward: -9596.763848005843\n",
            "Episode 1, Reward: -9624.961650875688\n",
            "Episode 1, Reward: -9212.215779347436\n",
            "Episode 1, Reward: -9511.768415837909\n",
            "Episode 1, Reward: -9566.451891670606\n",
            "Episode 1, Reward: -9052.888382956215\n",
            "Episode 1, Reward: -9338.542391332276\n",
            "Episode 1, Reward: -8880.237066764861\n",
            "Episode 1, Reward: -456.7306380369482\n",
            "Episode 1, Reward: -9127.565227367097\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -9338.0961993151\n",
            "Episode 1, Reward: -7172.655785126776\n",
            "Episode 1, Reward: -8933.875518508767\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -9625.613829679169\n",
            "Episode 1, Reward: -9625.371758696536\n",
            "Episode 1, Reward: -9391.361205013043\n",
            "Episode 1, Reward: -7985.530830538722\n",
            "Episode 1, Reward: -9341.443626842422\n",
            "Episode 1, Reward: -6182.184462287189\n",
            "Episode 1, Reward: 812.6623583184264\n",
            "Episode 1, Reward: -5968.889307365417\n",
            "Episode 1, Reward: -9425.911062634012\n",
            "Episode 1, Reward: -9597.617040490491\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -7552.90147469382\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -7719.934177210235\n",
            "Episode 1, Reward: -9384.84882013982\n",
            "Episode 1, Reward: -9205.225464777322\n",
            "Episode 1, Reward: -9602.633939294712\n",
            "Episode 1, Reward: -9426.111405769927\n",
            "Episode 1, Reward: -9440.496021374964\n",
            "Episode 1, Reward: -9328.371273203284\n",
            "Episode 1, Reward: -9315.850157550716\n",
            "Episode 1, Reward: -3987.7548608606808\n",
            "Episode 1, Reward: -8880.237066764861\n",
            "Episode 1, Reward: -9371.150517449447\n",
            "Episode 1, Reward: -8538.127410473477\n",
            "Episode 1, Reward: -9384.84882013982\n",
            "Episode 1, Reward: -9371.150517449447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters for synthetic data generation\n",
        "num_days = 100  # Number of days in the simulation\n",
        "initial_underlying_price = 100  # Starting price of the underlying asset\n",
        "volatility = 0.02  # Daily volatility for underlying asset\n",
        "option_sensitivity = 0.5  # Sensitivity of option price to changes in underlying\n",
        "initial_option_price = 10  # Starting price of the option\n",
        "\n",
        "# Generate synthetic underlying prices with random daily changes\n",
        "underlying_prices = [initial_underlying_price]\n",
        "for i in range(1, num_days):\n",
        "    daily_return = np.random.normal(0, volatility)\n",
        "    new_price = underlying_prices[-1] * (1 + daily_return)\n",
        "    underlying_prices.append(new_price)\n",
        "\n",
        "# Generate synthetic option prices with some correlation to underlying prices\n",
        "option_prices = [initial_option_price]\n",
        "for i in range(1, num_days):\n",
        "    # Calculate option price based on the underlying with added random noise\n",
        "    option_price = option_prices[-1] * (1 + option_sensitivity * (underlying_prices[i] - underlying_prices[i - 1]) / underlying_prices[i - 1])\n",
        "    option_price += np.random.normal(0, 0.5)  # Adding some noise\n",
        "    option_prices.append(option_price)\n",
        "\n",
        "# Combine into a DataFrame\n",
        "synthetic_data = pd.DataFrame({\n",
        "    'Day': np.arange(num_days),\n",
        "    'Underlying_Price': underlying_prices,\n",
        "    'Option_Price': option_prices\n",
        "})\n",
        "\n",
        "# Display the first few rows to check the synthetic dataset\n",
        "print(synthetic_data.head())\n",
        "\n",
        "# Save to CSV for later use\n",
        "synthetic_data.to_csv('synthetic_test_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "XT2eoaEgLp1z"
      },
      "id": "XT2eoaEgLp1z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model weights from the final saved model file\n",
        "def load_final_model(agent, filepath=final_model_path):\n",
        "    if os.path.exists(filepath):\n",
        "        agent.actor.load_state_dict(torch.load(filepath))\n",
        "        print(f\"Final model loaded from {filepath}\")\n",
        "    else:\n",
        "        print(\"No saved model found.\")\n",
        "\n",
        "# Testing function to run the model on a test dataset\n",
        "def test_ddpg(env, agent, test_simulations, num_episodes=10):\n",
        "    agent.actor.eval()  # Set the model to evaluation mode\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # Reset environment for each test episode\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Convert state to tensor and predict action\n",
        "            state_tensor = torch.FloatTensor(state).to(device)\n",
        "            with torch.no_grad():\n",
        "                action = agent.actor(state_tensor).cpu().numpy()\n",
        "\n",
        "            # Step in the environment using the selected action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # Summary of test results\n",
        "    avg_reward = np.mean(rewards)\n",
        "    print(f\"Average Reward over {num_episodes} test episodes: {avg_reward}\")\n",
        "    return avg_reward\n",
        "\n",
        "# Load the final model\n",
        "load_final_model(agent)\n",
        "\n",
        "# Define or load your test simulations data\n",
        "# Here, `test_simulations` should be defined similar to `simulations` but used for testing\n",
        "test_simulations = synthetic_data[['Underlying_Price', 'Option_Price']].values\n",
        "  # Replace with actual test data\n",
        "\n",
        "# Create a test environment with the test simulations\n",
        "test_env = DeltaHedgeEnv(test_simulations)\n",
        "\n",
        "# Run the test on the loaded model\n",
        "average_test_reward = test_ddpg(test_env, agent, test_simulations, num_episodes=10)\n",
        "print(f\"Average Test Reward: {average_test_reward}\")\n"
      ],
      "metadata": {
        "id": "hGif9wyWKDnl"
      },
      "id": "hGif9wyWKDnl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import pandas as pd\n",
        "\n",
        "# Load historical price data for the underlying asset\n",
        "underlying_data = pd.read_csv('underlying_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "# Load historical options data\n",
        "options_data = pd.read_csv('options_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "# Merge datasets on the Date column\n",
        "merged_data = pd.merge(underlying_data, options_data, on='Date', how='inner')\n",
        "\n",
        "# Select relevant columns\n",
        "# Assume 'Underlying_Price' and 'Option_Price' are columns in the merged dataset\n",
        "data = merged_data[['Date', 'Underlying_Price', 'Option_Price']]\n",
        "\n",
        "# Save the prepared dataset\n",
        "data.to_csv('prepared_test_data.csv', index=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jH9MlhBeLfKh"
      },
      "id": "jH9MlhBeLfKh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "name": "20241220.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}